{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "352ce062-ddde-47aa-b5e0-11ceecf219b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "175e0672-a0c4-4065-985d-31ada457a18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, X, y, learning_rate=0.1):\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # --- Output Layer Gradients ---\n",
    "        # Derivative of Loss w.r.t Z2 (pred - target)\n",
    "        dZ2 = self.A2 - y\n",
    "        # Derivative w.r.t W2\n",
    "        dW2 = (1 / m) * np.dot(self.A1.T, dZ2)\n",
    "        # Derivative w.r.t b2\n",
    "        db2 = (1 / m) * np.sum(dZ2, axis=0, keepdims=True)\n",
    "        \n",
    "        # --- Hidden Layer Gradients ---\n",
    "        # Derivative of Sigmoid: s * (1 - s)\n",
    "        d_sigmoid_Z1 = self.A1 * (1 - self.A1)\n",
    "        \n",
    "        # Backpropagate error to hidden layer\n",
    "        dZ1 = np.dot(dZ2, self.W2.T) * d_sigmoid_Z1\n",
    "        \n",
    "        dW1 = (1 / m) * np.dot(X.T, dZ1)\n",
    "        db1 = (1 / m) * np.sum(dZ1, axis=0, keepdims=True)\n",
    "        \n",
    "        # --- Update Weights (Gradient Descent) ---\n",
    "        self.W2 = self.W2 - learning_rate * dW2\n",
    "        self.b2 = self.b2 - learning_rate * db2\n",
    "        self.W1 = self.W1 - learning_rate * dW1\n",
    "        self.b1 = self.b1 - learning_rate * db1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd74e92-fe79-4eda-ae49-5b25db6ef2e9",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "### You can now train the MLP to solve XOR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "450e966a-c30d-4820-a095-01493ce27522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- 1. Define the MLP Class ---\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights with small random values\n",
    "        # Layer 1: Input -> Hidden\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        \n",
    "        # Layer 2: Hidden -> Output\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # --- Layer 1 ---\n",
    "        self.Z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.A1 = self.sigmoid(self.Z1)  # Activation Layer 1\n",
    "        \n",
    "        # --- Layer 2 ---\n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        self.A2 = self.sigmoid(self.Z2)  # Activation Layer 2 (Output)\n",
    "        \n",
    "        return self.A2\n",
    "\n",
    "    def calculate_loss(self, y_true, y_pred):\n",
    "        # Binary Cross Entropy Loss\n",
    "        m = y_true.shape[0]\n",
    "        epsilon = 1e-15 # To prevent log(0) error\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        loss = -1/m * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return loss\n",
    "\n",
    "    def backward(self, X, y, learning_rate=0.1):\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # --- Output Layer Gradients ---\n",
    "        dZ2 = self.A2 - y\n",
    "        dW2 = (1 / m) * np.dot(self.A1.T, dZ2)\n",
    "        db2 = (1 / m) * np.sum(dZ2, axis=0, keepdims=True)\n",
    "        \n",
    "        # --- Hidden Layer Gradients ---\n",
    "        d_sigmoid_Z1 = self.A1 * (1 - self.A1) # Derivative of Sigmoid\n",
    "        dZ1 = np.dot(dZ2, self.W2.T) * d_sigmoid_Z1\n",
    "        \n",
    "        dW1 = (1 / m) * np.dot(X.T, dZ1)\n",
    "        db1 = (1 / m) * np.sum(dZ1, axis=0, keepdims=True)\n",
    "        \n",
    "        # --- Update Weights ---\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcd95177-5c66-4ea5-8e10-26662cde2e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n",
      "Epoch 0, Loss: 0.6932\n",
      "Epoch 1000, Loss: 0.6931\n",
      "Epoch 2000, Loss: 0.6931\n",
      "Epoch 3000, Loss: 0.6931\n",
      "Epoch 4000, Loss: 0.6931\n",
      "Epoch 5000, Loss: 0.6931\n",
      "Epoch 6000, Loss: 0.6931\n",
      "Epoch 7000, Loss: 0.6931\n",
      "Epoch 8000, Loss: 0.6931\n",
      "Epoch 9000, Loss: 0.6931\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Prepare Data (XOR Problem) ---\n",
    "# XOR Logic: 0,0->0 | 0,1->1 | 1,0->1 | 1,1->0\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# --- 3. Initialize Model ---\n",
    "# Input: 2 features, Hidden: 4 neurons, Output: 1 neuron\n",
    "mlp = MLP(input_size=2, hidden_size=4, output_size=1)\n",
    "\n",
    "# --- 4. Training Loop ---\n",
    "print(\"Training started...\")\n",
    "for epoch in range(10000):\n",
    "    # Forward pass\n",
    "    output = mlp.forward(X)\n",
    "    \n",
    "    # Backward pass & Weight Update\n",
    "    mlp.backward(X, y, learning_rate=0.5) # Increased LR slightly for faster convergence\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        loss = mlp.calculate_loss(y, output)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65b7d57d-c08e-4b9c-8612-b0196c6f6d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Predictions (Raw Probabilities):\n",
      "[[0.4999928 ]\n",
      " [0.50000682]\n",
      " [0.49999319]\n",
      " [0.5000072 ]]\n",
      "\n",
      "Final Predictions (Rounded):\n",
      "[[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Final Prediction ---\n",
    "print(\"\\nFinal Predictions (Raw Probabilities):\")\n",
    "print(mlp.forward(X)) \n",
    "\n",
    "print(\"\\nFinal Predictions (Rounded):\")\n",
    "print(np.round(mlp.forward(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20db9fd-5854-4b92-b138-6203270abf57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
